2020-09-20 11:29:56,823 ERROR main org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:806) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:776) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:649) [hadoop-common-2.7.5.jar:na]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at edu.gatech.cse6250.helper.SparkHelper$.createSparkSession(SparkHelper.scala:39) [big-data-hw3_2.11-1.1.0.jar:1.1.0]
	at edu.gatech.cse6250.helper.SparkHelper$.spark$lzycompute(SparkHelper.scala:21) [big-data-hw3_2.11-1.1.0.jar:1.1.0]
	at edu.gatech.cse6250.helper.SparkHelper$.spark(SparkHelper.scala:14) [big-data-hw3_2.11-1.1.0.jar:1.1.0]
	at edu.gatech.cse6250.main.Main$.main(Main.scala:30) [big-data-hw3_2.11-1.1.0.jar:1.1.0]
	at edu.gatech.cse6250.main.Main.main(Main.scala) [big-data-hw3_2.11-1.1.0.jar:1.1.0]
2020-09-20 11:29:56,841 WARN main org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 11:29:59,574 WARN Executor task launch worker for task 98 com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2020-09-20 11:29:59,614 INFO Executor task launch worker for task 98 com.github.fommil.jni.JniLoader - successfully loaded C:\Users\ktlco\AppData\Local\Temp\jniloader3493014455986236775netlib-native_ref-win-x86_64.dll
2020-09-20 11:29:59,871 WARN main com.github.fommil.netlib.LAPACK - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
2020-09-20 11:29:59,883 INFO main com.github.fommil.jni.JniLoader - already loaded netlib-native_ref-win-x86_64.dll
2020-09-20 11:30:18,978 ERROR pool-1-thread-1 org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:806) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:776) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:649) [hadoop-common-2.7.5.jar:na]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292) [spark-core_2.11-2.3.0.jar:2.3.0]
	at edu.gatech.cse6250.util.LocalClusterSparkContext$class.beforeAll(LocalClusterSparkContext.scala:27) [test-classes/:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.beforeAll(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187) [scalatest_2.11-2.2.5.jar:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.beforeAll(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253) [scalatest_2.11-2.2.5.jar:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.run(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:466) [scalatest_2.11-2.2.5.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:677) [scalatest_2.11-2.2.5.jar:na]
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304) [test-agent-1.3.13.jar:1.3.13]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.8.0_261]
	at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_261]
2020-09-20 11:30:18,991 WARN pool-1-thread-1 org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 11:30:20,461 WARN pool-1-thread-1-ScalaTest-running-LoadRddRawDataTest org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2020-09-20 11:33:45,777 ERROR main org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:806) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:776) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:649) [hadoop-common-2.7.5.jar:na]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921) [spark-sql_2.11-2.3.0.jar:2.3.0]
	at edu.gatech.cse6250.helper.SparkHelper$.createSparkSession(SparkHelper.scala:39) [classes/:na]
	at edu.gatech.cse6250.helper.SparkHelper$.spark$lzycompute(SparkHelper.scala:21) [classes/:na]
	at edu.gatech.cse6250.helper.SparkHelper$.spark(SparkHelper.scala:14) [classes/:na]
	at $line15.$read$$iw$$iw$.<init>(<console>:22) [scala-library-2.11.12.jar:na]
	at $line15.$read$$iw$$iw$.<clinit>(<console>) [scala-library-2.11.12.jar:na]
	at $line15.$eval$.$print$lzycompute(<console>:7) [scala-library-2.11.12.jar:na]
	at $line15.$eval$.$print(<console>:6) [scala-library-2.11.12.jar:na]
	at $line15.$eval.$print(<console>) [scala-library-2.11.12.jar:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_261]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_261]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_261]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_261]
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644) [scala-compiler-2.11.12.jar:na]
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) [scala-reflect-2.11.12.jar:na]
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19) [scala-reflect-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:819) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:691) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:404) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:425) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:993) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:891) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:891) [scala-compiler-2.11.12.jar:na]
	at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97) [scala-reflect-2.11.12.jar:na]
	at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:891) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:74) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.MainGenericRunner.run$1(MainGenericRunner.scala:87) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:98) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:103) [scala-compiler-2.11.12.jar:na]
	at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) [scala-compiler-2.11.12.jar:na]
2020-09-20 11:33:45,797 WARN main org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 11:37:53,809 INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2020-09-20 11:39:25,922 INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2020-09-20 11:40:27,918 INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2020-09-20 11:43:27,007 INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2020-09-20 11:54:41,710 INFO main org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2020-09-20 11:58:23,611 ERROR Executor task launch worker for task 7 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 7.0 (TID 7)
java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/ktlco/OneDrive/Documents/MSA/CSE6250/hw3/hw3/code/data/encounter_INPUT.csv; isDirectory=false; length=98522811; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:527) ~[spark-sql_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:514) ~[spark-sql_2.11-2.3.0.jar:2.3.0]
	at scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152) ~[scala-library-2.11.12.jar:na]
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[scala-library-2.11.12.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[scala-library-2.11.12.jar:na]
Caused by: java.lang.RuntimeException: file:/C:/Users/ktlco/OneDrive/Documents/MSA/CSE6250/hw3/hw3/code/data/encounter_INPUT.csv is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [111, 110, 115, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476) ~[parquet-hadoop-1.8.2.jar:1.8.2]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445) ~[parquet-hadoop-1.8.2.jar:1.8.2]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421) ~[parquet-hadoop-1.8.2.jar:1.8.2]
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:520) ~[spark-sql_2.11-2.3.0.jar:2.3.0]
	... 32 common frames omitted
2020-09-20 11:58:23,628 WARN task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/ktlco/OneDrive/Documents/MSA/CSE6250/hw3/hw3/code/data/encounter_INPUT.csv; isDirectory=false; length=98522811; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:527)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:514)
	at scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)
	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)
	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
	at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
	at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
	at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)
	at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)
	at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: file:/C:/Users/ktlco/OneDrive/Documents/MSA/CSE6250/hw3/hw3/code/data/encounter_INPUT.csv is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [111, 110, 115, 10]
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:520)
	... 32 more

2020-09-20 11:58:23,630 ERROR task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 7.0 failed 1 times; aborting job
2020-09-20 12:03:01,032 WARN main org.apache.spark.util.Utils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2020-09-20 13:14:48,353 ERROR Executor task launch worker for task 77 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 35.0 (TID 77)
java.lang.ClassCastException: java.lang.String cannot be cast to java.sql.Date
	at org.apache.spark.sql.Row$class.getDate(Row.scala:269) ~[spark-catalyst_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDate(rows.scala:166) ~[spark-catalyst_2.11-2.3.0.jar:2.3.0]
	at $line88.$read$$iw$$iw$$anonfun$1.apply(<console>:23) ~[scala-library-2.11.12.jar:na]
	at $line88.$read$$iw$$iw$$anonfun$1.apply(<console>:23) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.to(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:109) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_261]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_261]
2020-09-20 13:14:48,361 WARN task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 35.0 (TID 77, localhost, executor driver): java.lang.ClassCastException: java.lang.String cannot be cast to java.sql.Date
	at org.apache.spark.sql.Row$class.getDate(Row.scala:269)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDate(rows.scala:166)
	at $line88.$read$$iw$$iw$$anonfun$1.apply(<console>:23)
	at $line88.$read$$iw$$iw$$anonfun$1.apply(<console>:23)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-09-20 13:14:48,363 ERROR task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 35.0 failed 1 times; aborting job
2020-09-20 13:23:32,420 ERROR Executor task launch worker for task 82 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 40.0 (TID 82)
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101) ~[scala-library-2.11.12.jar:na]
	at $line104.$read$$iw$$iw$$anonfun$1.apply(<console>:25) ~[scala-library-2.11.12.jar:na]
	at $line104.$read$$iw$$iw$$anonfun$1.apply(<console>:25) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.to(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:109) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_261]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_261]
2020-09-20 13:23:32,421 WARN task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 40.0 (TID 82, localhost, executor driver): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)
	at $line104.$read$$iw$$iw$$anonfun$1.apply(<console>:25)
	at $line104.$read$$iw$$iw$$anonfun$1.apply(<console>:25)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-09-20 13:23:32,422 ERROR task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 40.0 failed 1 times; aborting job
2020-09-20 13:25:49,698 ERROR Executor task launch worker for task 84 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 42.0 (TID 84)
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114) ~[scala-library-2.11.12.jar:na]
	at $line110.$read$$iw$$iw$$anonfun$1.apply(<console>:25) ~[scala-library-2.11.12.jar:na]
	at $line110.$read$$iw$$iw$$anonfun$1.apply(<console>:25) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394) ~[scala-library-2.11.12.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) ~[scala-library-2.11.12.jar:na]
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.to(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289) ~[scala-library-2.11.12.jar:na]
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334) ~[scala-library-2.11.12.jar:na]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:109) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) ~[spark-core_2.11-2.3.0.jar:2.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_261]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_261]
2020-09-20 13:25:49,699 WARN task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 42.0 (TID 84, localhost, executor driver): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at $line110.$read$$iw$$iw$$anonfun$1.apply(<console>:25)
	at $line110.$read$$iw$$iw$$anonfun$1.apply(<console>:25)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:394)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2020-09-20 13:25:49,707 ERROR task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 42.0 failed 1 times; aborting job
2020-09-20 13:31:29,979 ERROR pool-1-thread-1 org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:390) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:806) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:776) [hadoop-common-2.7.5.jar:na]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:649) [hadoop-common-2.7.5.jar:na]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.12.jar:na]
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464) [spark-core_2.11-2.3.0.jar:2.3.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292) [spark-core_2.11-2.3.0.jar:2.3.0]
	at edu.gatech.cse6250.util.LocalClusterSparkContext$class.beforeAll(LocalClusterSparkContext.scala:27) [test-classes/:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.beforeAll(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187) [scalatest_2.11-2.2.5.jar:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.beforeAll(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253) [scalatest_2.11-2.2.5.jar:na]
	at edu.gatech.cse6250.main.LoadRddRawDataTest.run(LoadRddRawDataTest.scala:9) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:466) [scalatest_2.11-2.2.5.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:677) [scalatest_2.11-2.2.5.jar:na]
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304) [test-agent-1.3.13.jar:1.3.13]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.8.0_261]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.8.0_261]
	at java.lang.Thread.run(Unknown Source) ~[na:1.8.0_261]
2020-09-20 13:31:29,991 WARN pool-1-thread-1 org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 13:31:31,120 WARN pool-1-thread-1 org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:31,405 WARN pool-1-thread-1-ScalaTest-running-LoadRddRawDataTest org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2020-09-20 13:31:34,584 WARN pool-1-thread-1-ScalaTest-running-LoadRddRawDataTest org.apache.spark.util.Utils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2020-09-20 13:31:37,456 WARN pool-1-thread-1 org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:37,576 WARN pool-1-thread-1-ScalaTest-running-T2dmPhenotypeTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,471 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,554 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,626 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,683 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,740 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,792 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,845 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,899 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:38,955 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,028 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,081 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,134 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,192 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,252 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,319 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,375 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:31:39,454 WARN pool-1-thread-1-ScalaTest-running-FeatureConstructionTest org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-20 13:39:39,955 WARN pool-1-thread-1 org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 13:39:41,304 WARN pool-1-thread-1-ScalaTest-running-LoadRddRawDataTest org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2020-09-20 13:39:44,306 WARN pool-1-thread-1-ScalaTest-running-LoadRddRawDataTest org.apache.spark.util.Utils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2020-09-20 13:45:26,781 WARN ScalaTest-run org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-20 13:45:28,400 WARN ScalaTest-run-running-LoadRddRawDataTest org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2020-09-20 13:45:32,094 WARN ScalaTest-run-running-LoadRddRawDataTest org.apache.spark.util.Utils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
